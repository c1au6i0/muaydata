---
title: Snakes, pandas, sea born creatures and how to find them.
author: c1au6io_hh
date: '2019-05-06'
slug: reticulate
categories:
  - R
  - Python
tags:
  - reticulate
  - import_csv
  - microbenchmark
  - seaborn
lastmod: '2019-05-06T11:46:00-04:00'
keywords: []
description: ''
comment: yes
toc: yes
autoCollapseToc: yes
contentCopyright: yes
reward: no
mathjax: no
---

What's the fasted way to read a csv file in `R`? Among the `R` packages, the ultra-fast sprinter is certainly `data.table` but...few years ago, the introduction of the package [`reticulate`](https://blog.rstudio.com/2018/03/26/reticulate-r-interface-to-python/) gave us the possibility of use `python` and in particular the library `pandas` to read files in `R`. Would `pandas` (used in `R`) be faster than `data.table`? How would its performance compare with `readr` or base `R`? Let's take a look.

<!--more-->

# Benchmark evaluation

We  start by loading few `R` libraries to read the csv file and  evaluate the performance (`microbenchmark`) of our contestants...

```{r r_libraries, message=FALSE}
library(tidyverse)
library(reticulate)
library(data.table)
library(microbenchmark)
```

and by getting `pandas` in the `python` environment.

```{python pandas}
import pandas as pd
```

I downloaded the [dataset](https://vincentarelbundock.github.io/Rdatasets/csv/boot/amis.csv) that consists of 8437	observations of 4 variables and I placed on my home folder.

```{r load_mb, include=FALSE, eval=TRUE}
mb <-  readRDS("/Users/heverz/Documents/R_projects/muaydata/static/data_post5/benchmark.RDS")
# 
# saveRDS(mb,"/Users/heverz/Documents/R_projects/muaydata/static/data_post5/benchmark.RDS")
```

We are going to read the csv file using:

* base *R* function `read.csv`
* `pandas` function `read_csv`
* `readr::read_csv`
* `data.table::fread`

We read the file  1000 times for each of the package and record the performance with `microbenchmark::microbenchmark`

```{r microbench, include=TRUE, eval=FALSE, message=FALSE}
mb <- microbenchmark(
          "base" = {
            read.csv("~/amis.csv", sep=",")
          },
          "readr" = {
            read_csv("~/amis.csv")
          },
          "pandas" = {py_run_string("pd.read_csv('~/amis.csv')")
          },
          "data.table" = {
            fread("~/amis.csv")
          },
          times = 1000)
```

And we summarize it
```{r, message=FALSE, warning=FALSE}
mb %>% 
  group_by(expr) %>% 
  rename(package = expr) %>% 
  mutate(time_ms = time * 0.000001) %>% 
  summarize(mean = mean(time_ms), median = median(time_ms), min = min(time_ms), max = max(time_ms), sd = sd(time_ms)) %>% 
  arrange(mean) %>% 
  knitr::kable(format = "html", caption = ": Milliseconds to read a csv file")

```

The faster function is still `data.table::fread` with a mean reading time of about ~1 ms,  followed by `pandas` (4.35 ms) and `readr` (4.81 ms). The R base function `read.csv` is the slowest, with reading times about 4-fold  larger than `data.table::fread`.

We could graphically visualize the `microbenchmark` performance just launching `autoplot(mb)` but that would not be fun! We come so far, why not visualize the data using `python`?

# Seaborn

First we need to convert the *R* object `mb` to `python` object.
```{r to_py, fig.align="center"}
py$mb <- r_to_py(mb, convert = TRUE)
```

In what is dataframe converted in *R*?

```{python type_py}
type(mb)
```
...of course in a `pandas Dataframe`.

Now, let's import some libraries and plot the data using `seaborn`
```{python p_libraries}
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

sns.set(style="whitegrid", palette="muted")

mb['time_ms'] = mb['time'] * 0.000001 # from nano seconds to millisecond

ax = sns.boxplot(x="expr", y="time_ms", data = mb)

ax  = ax.set(ylabel='Time (milliseconds)', xlabel='package')

plt.show()
```

# Conclusion

In this sprint race to import csv in *R*, the first place is still hold by the favourite `data.table::fread` followed by `pandas read_csv` and then by `readr::read_csv`. These two last packages/functions were really close at the final line. The base `R` function `read.csv` was not able to get to the podium and had reading times about 4-fold larger than `data.table::fread`.

Ciao Ciao!

