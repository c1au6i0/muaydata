---
title: "Post 4"
output: html_notebook
---
We start loading the libraries.
```{r libraries, message=FALSE}
library(svDialogs)
library(tidyverse)
library(purrr)
library(foreign) 
library(RSQLite)
library(caret)
library(glmnet)
# MLmetrics
```

ADD Health
Where are the data:
[link1](https://www.icpsr.umich.edu/icpsrweb/content/DSDR/add-health-data-guide.html)
[link2](https://www.cpc.unc.edu/projects/addhealth/documentation/ace/tool/topics)




```{r getdata, eval=FALSE}

files <- list.files(path = "/Users/heverz/Documents/R_projects/DATASETS/W1-4",
                    include.dirs=FALSE,
                    full.names = TRUE)

w1_4 <- map(files, read.dta)

var_select <- scan("/Users/heverz/Documents/R_projects/muaydata/static/data-post4/variables.csv", what  = "list", sep = ",")

# get the varibles in var_select
w1_4_reduced <- map(w1_4, function (x) x[, colnames(x) %in% var_select])

#join not empty dataframes
toselect <- which(map(w1_4_reduced, function(x) ncol(x) > 0) == TRUE)
heal_data <- plyr::join_all(w1_4_reduced[toselect] , by = "AID", type = "inner")
heal_data$AID <- as.character(heal_data$AID)
```

```{r database, include=FALSE, eval=TRUE}
mydb <- dbConnect(RSQLite::SQLite(), "/Users/heverz/Documents/R_projects/muaydata/static/data-post4/heal_data.sqlite")


# dbWriteTable(mydb, "heal_data", heal_data)
# dbListTables(mydb)
heal_data <- as_tibble(tbl(mydb, "heal_data"))
```




```{r factor}
heal_sub <- heal_data[,2:length(heal_data)]
heal_sub <- mutate_if(heal_sub, is.character, as.factor)
sum(is.na(heal_sub))
str(heal_sub)
```

There are a lot of NA because the questionare was not submitted to a some parents
```{r NAs}
glimpse(heal_sub) 

heal_sub[is.na(heal_sub$PA10),]
```

Parent questionare is an important part of our analysis we will remove those NAs

```{r removeNAs}
heal_sub_cl <- 
  heal_sub %>% 
  select(-c("S10")) %>% 
  select(1:48,51) %>% 
  na.omit()

names(heal_sub_cl)
#classifier_NA_insert
heal_sub_cl <- 
  heal_sub_cl %>% 
  mutate(H4TO117 = as.factor(str_extract(heal_sub_cl$H4TO117 , "[:alpha:]+")))
summary(heal_sub_cl$H4TO117)
```

https://www.analyticsvidhya.com/blog/2016/12/practical-guide-to-implement-machine-learning-with-caret-package-in-r-with-practice-problem/


```{r nzv}
var_nz <- names(heal_sub_cl)[nearZeroVar(heal_sub_cl[,-49], saveMetrics= FALSE)]

nearZeroVar(heal_sub_cl[,-49], saveMetrics = TRUE,) %>% 
  filter(nzv == TRUE) %>% 
  mutate( name_v = var_nz)
```



https://topepo.github.io/caret/available-models.html

The problem is that we have a very unballanced designed and therefore all the models tend to maximaze accuracy but not specificity (they say all not addicted). So we need to consider specificity as a metric too

# A DIFFERENT CUT OFF and randomforest
```{r create_my_model}
## Get the model code for the original random forest method:

thresh_code <- getModelInfo("rf", regex = FALSE)[[1]]
thresh_code$type <- c("Classification")
## Add the threshold as another tuning parameter
thresh_code$parameters <- data.frame(parameter = c("mtry", "threshold"),
                                     class = c("numeric", "numeric"),
                                     label = c("#Randomly Selected Predictors",
                                               "Probability Cutoff"))
## The default tuning grid code:
thresh_code$grid <- function(x, y, len = NULL, search = "grid") {
  p <- ncol(x)
  if(search == "grid") {
    grid <- expand.grid(mtry = floor(sqrt(p)), 
                        threshold = seq(.01, .99, length = len))
    } else {
      grid <- expand.grid(mtry = sample(1:p, size = len),
                          threshold = runif(1, 0, size = len))
      }
  grid
  }

## Here we fit a single random forest model (with a fixed mtry)
## and loop over the threshold values to get predictions from the same
## randomForest model.
thresh_code$loop = function(grid) {   
  library(plyr)
  loop <- ddply(grid, c("mtry"),
                function(x) c(threshold = max(x$threshold)))
  submodels <- vector(mode = "list", length = nrow(loop))
  for(i in seq(along = loop$threshold)) {
    index <- which(grid$mtry == loop$mtry[i])
    cuts <- grid[index, "threshold"] 
    submodels[[i]] <- data.frame(threshold = cuts[cuts != loop$threshold[i]])
    }    
  list(loop = loop, submodels = submodels)
  }

## Fit the model independent of the threshold parameter
thresh_code$fit = function(x, y, wts, param, lev, last, classProbs, ...) { 
  if(length(levels(y)) != 2)
    stop("This works only for 2-class problems")
  randomForest(x, y, mtry = param$mtry, ...)
  }

## Now get a probability prediction and use different thresholds to
## get the predicted class
thresh_code$predict = function(modelFit, newdata, submodels = NULL) {
  class1Prob <- predict(modelFit, 
                        newdata, 
                        type = "prob")[, modelFit$obsLevels[1]]
  ## Raise the threshold for class #1 and a higher level of
  ## evidence is needed to call it class 1 so it should 
  ## decrease sensitivity and increase specificity
  out <- ifelse(class1Prob >= modelFit$tuneValue$threshold,
                modelFit$obsLevels[1], 
                modelFit$obsLevels[2])
  if(!is.null(submodels)) {
    tmp2 <- out
    out <- vector(mode = "list", length = length(submodels$threshold))
    out[[1]] <- tmp2
    for(i in seq(along = submodels$threshold)) {
      out[[i+1]] <- ifelse(class1Prob >= submodels$threshold[[i]],
                           modelFit$obsLevels[1], 
                           modelFit$obsLevels[2])
      }
    } 
  out  
  }

## The probabilities are always the same but we have to create
## mulitple versions of the probs to evaluate the data across
## thresholds
thresh_code$prob = function(modelFit, newdata, submodels = NULL) {
  out <- as.data.frame(predict(modelFit, newdata, type = "prob"))
  if(!is.null(submodels)) {
    probs <- out
    out <- vector(mode = "list", length = length(submodels$threshold)+1)
    out <- lapply(out, function(x) probs)
    } 
  out 
  }
```



```{r tuning}

fourStats <- function (data, lev = levels(data$obs), model = NULL) {
  ## This code will get use the area under the ROC curve and the
  ## sensitivity and specificity values using the current candidate
  ## value of the probability threshold.
  out <- c(twoClassSummary(data, lev = levels(data$obs), model = NULL))
  
  ## The best possible model has sensitivity of 1 and specificity of 1. 
  ## How far are we from that value?
  coords <- matrix(c(1, 1, out["Spec"], out["Sens"]), 
                   ncol = 2, 
                   byrow = TRUE)
  colnames(coords) <- c("Spec", "Sens")
  rownames(coords) <- c("Best", "Current")
  c(out, Dist = dist(coords)[1])
}



glmnetControl <- trainControl(method = "repeatedcv", 
                              number = 10,
                              repeats = 10,
                              savePredictions = TRUE,
                              summaryFunction = multiClassSummary,
                              classProbs = TRUE,
                              search = "grid"
                              )
```



```{r model}

# 
 
heal_sub_cl$H4TO117[heal_sub_cl$H4TO117 == "Legitimate"] <- "No"
# 
# 
heal_sub_cl$H4TO117 <- droplevels(heal_sub_cl$H4TO117)

summary(heal_sub_cl$H4TO117)


mod1 <- train(H4TO117  ~ ., 
              heal_sub_cl,
              method = thresh_code,
              ## Minimize the distance to the perfect model
              metric = "Dist",
              maximize = FALSE,
              tuneLength = 20,
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = TRUE,
                                       summaryFunction = fourStats
                                       ),
              na.action = na.omit,
              preProcess = c("nzv","pca")
              )

plot(mod1)

saveRDS(mod1, "mod1.RDS")


```



https://stackoverflow.com/questions/53654193/how-to-generate-confusion-matrix-on-hold-out-sample-in-caret-xgbdart

https://www.svm-tutorial.com/2014/11/svm-classify-text-r/

https://stackoverflow.com/questions/35482094/glmnet-caret-roc-sensitivity-specificity-of-training-model


http://appliedpredictivemodeling.com/blog/2014/2/1/lw6har9oewknvus176q4o41alqw2ow

```{r}
library(reshape2)
metrics <- mod1$results[, c(2, 4:6)]
metrics <- melt(metrics, id.vars = "threshold", 
                variable.name = "Resampled",
                value.name = "Data")

ggplot(metrics, aes(x = threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top")

confusionMatrix(mod1)

attr(mod1)

```

```{r adaptative}
# fitControl <- trainControl(method = "adaptive_cv",
#                            number = 10, repeats = 10,
#                            summaryFunction = twoClassSummary,
#                            classProbs = TRUE,
#                            adaptive = list(min = 2,
#                                            alpha = 0.05,
#                                            method = "gls",
#                                            complete = TRUE),
#                            search = "random")
```




# RESAMPLING

