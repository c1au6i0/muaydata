---
title: "Cassandra, Aceso and the trees"
output: html_notebook
---

In this post we will apply predicting modelling to data of [__ADD HEALTH__](https://www .cpc.unc.edu/projects/addhealth/about), a national longitudinal study that enrolled  ~90000 adolescents and followed them up for 20 years. Using the *R* package [```caret```](https://github.com/topepo/caret), we will train and evaluate different models (linear, decison tree, random Forrest) in order to predict drug use behavior in adulthood from information (friends, social activities, parents and others) collected during adolescence. Doing that will give us the opportunity to play and get familiarized with the most important ```caret``` functions as well as introduce seminal concepts of machine learning.

# The Data

__ADD HEALTH__ is a unique study both in terms of the extent of information collected ("the largest, most comprehensive longitudinal survey of adolescents ever undertaken") and duration (started in 1994 and is still on going [study design](https://www.cpc.unc.edu/projects/addhealth/design/designpage001.jpg/@@images/b842a2e4-e345-4384-bfc5-c724a3b55b5c.jpeg)).

We will analyze the  [publicly available data](https://www.cpc.unc.edu/projects/addhealth/documentation/publicdata), a small fraction of the total subjects surveyed ~5%, but still a quite decent dataset of ~ 6000 observations X several thousand variables.

It is important to keep in mind that the __current analysis will be performed on unweighted data__ (ADD  sampling design is quite complex) and models will be evaluated in samples not necessaries representative of the population. __This can affect the final estimates of the model effectiveness__. 

Users interested in the specific statistical procedures suggested for ADD Health weight adjustment can find info in this [guide](https://www.cpc.unc.edu/projects/addhealth/documentation/guides/wt_guidelines_20161213.pdf) and this [article](https://www.jstor.org/stable/2985969?seq=1#page_scan_tab_contents).

# Obtain the dataset and make it tidy

We start loading some libraries.
```{r libraries, message=FALSE}
library(tidyverse)
library(purrr)
library(foreign) 
library(RSQLite)
library(svDialogs)
library(caret)
library(glmnet)
library(randomForest)
library(broom)
library(rpart)
library(party)
library(MLmetrics)
library(PRROC)
```


```{r load_all, eval= TRUE, include=FALSE}
mod_glmnet <- readRDS("mod_glmnet_f.RDS")
mod_rpart <- readRDS("mod_rpart_f.RDS")
mod_ctree <- readRDS("mod_ctree_f.RDS")
mod_ranger <- readRDS("mod_ranger_all.RDS")

confusionTable <- function (mod) {
  tidy(confusionMatrix(mod)$table) %>% 
  mutate( model = mod$method) %>% 
  spread(model, n) %>% 
  mutate(Case = c("True Negative", 
                   "False Negative",
                   "False Postive", 
                   "True Positive")) %>% 
  arrange(Case) %>% 
  select(Case, everything()) %>% 
  mutate_if(is.numeric, round, 2)
}


table_m4  <-  bind_cols(confusionTable(mod_glmnet),
                confusionTable(mod_rpart)[,4],
                confusionTable(mod_ctree)[,4],
                confusionTable(mod_ranger)[,4])
table_m4 
```

The number of variables collected for the ADD HEALTH is of several thousands. We will focus our analysis on most of the variables collected in the first wave (first survey in 1994) that include demographic information, friend and social network details, parental information and many others) and we will select one classifier from the fourth wave (2006). 

A description of all the variables of the study can be found using the [codebook web Explorer](https://www.cpc.unc.edu/projects/addhealth/documentation/ace/tool/topics). 

I downloaded the public-use data from [ICPSR](https://www.icpsr.umich.edu/icpsrweb/DSDR/studies/21600#) and then saved all the files with *dta* (Stata) extension in a folder. At this point, we can import all the files, select the columns that are in var_select and join the dataframes together by the subject identification variable (```AID``` that then we remove). Finaly, all the the columns that contain character data are transformed to factors.

```{r getdata}
files <- list.files(path = "/Users/heverz/Documents/R_projects/DATASETS/W1-4",
                    include.dirs=FALSE,
                    full.names = TRUE)

# import all the dataframes.
w1_4 <- map(files, read.dta)

# identify the name of varibles of interest
h1_pa <- names(w1_4[[1]])[str_detect(names(w1_4[[1]]), '^H1|PA')] # H1 is the first wave 
ids <- c("AID", "GSWGT4_2", "CLUSTER2", "H4TO117") # H4 is the classifier and GSW are the weights
var_select <- append(h1_pa, ids)

# select the variables
w1_4_reduced <- map(w1_4, function (x) x[, colnames(x) %in% var_select])

# join not empty dataframes
toselect <- which(map(w1_4_reduced, function(x) ncol(x) > 0) == TRUE) # list of not empty dataframes
heal_data <- plyr::join_all(w1_4_reduced[toselect] , by = "AID", type = "inner")

# two colums same name /remove one
heal_data <- heal_data[, -2337] 
names(heal_data)[2335] <- "CLUSTER2"

# character columns are factors
heal_sub <- mutate_if(heal_data, is.character, as.factor)
```

Some columns are factors with only one level. We are going to remove them.

```{r remove_1factor}
# some columns have only one level
nlevel_count <- map_int(heal_sub, nlevels)
lev_1 <- unlist(attributes(nlevel_count[nlevel_count == 1]))

heal_sub <- heal_sub[,  !names(heal_sub) %in% lev_1]
```

Same columns miss quite a lot of data. We can *impute* missing observation only if there are not too many in each column. We are going to drop all the columns that have more than 15% of missing data.

```{r NAs}
na_byc <-  unlist(map(heal_sub, function(x) sum(is.na(x))))

# if more than 15% NA in column then not take the column
to_remove <- na_byc[na_byc >= nrow(heal_sub)*0.15]  

# subset
heal_sub <- heal_sub[,!names(heal_sub) %in% unlist(attributes(to_remove))]
```

The _*H4TO117*_ column is our classifier, the response to the following question:

*"Have you ever continued to use {favorite drug} after you realized using {favorite drug} was causing you any emotional problems (such as feeling depressed or empty, feeling irritable or aggressive, feeling paranoid or confused, feeling anxious or tense, being jumpy or easily startled) or causing you any health problems (such as heart pounding, headaches or dizziness, or sexual difficulties)?"*

Let's clean it up and see the responses.

```{r select_classifier}
#classifier
heal_sub_cl <- 
  heal_sub %>%
  select(-c("AID","GSWGT4_2", "CLUSTER2", "H1GI1M")) %>%  # remove column of weight, month of birth, and clusters 
  mutate(H4TO117 = as.factor(str_extract(H4TO117 , "[:alpha:]+")))  # keep only letters

```

*Legitimate* stands for Legitimate skips, that indicates those people that do not have a favorite drug.

The answers to this question not only allow to distinguish people that use (or had used drugs) from people that did not (legitimate skip), but also to identify those individuals that continue to take drugs despite negative consequences (yes). The class "yes" will be our class of interest because it indicates those people that are presumably more likely to develop health problems as a consequence of drug use patterns. 

Therefore, we will try first to  fit 2 class models, before trying something more complex. We replace "Legitimate" with "No" to obtain 2 classes: those that continued to take drugs despite negative consequences (*yes*) and those that did not (*no*).

```{r 2Classes}
heal_sub_cl$H4TO117[heal_sub_cl$H4TO117 == "Legitimate"] <- "No"

heal_sub_cl$H4TO117 <- droplevels(heal_sub_cl$H4TO117)
 
heal_sub_cl$H4TO117 <- factor(heal_sub_cl$H4TO117, levels = c("Yes", "No")) # class of interest need to  be the first

summary(heal_sub_cl$H4TO117)
```

#   The Development of the Models

## Cross Validation, Class Inbalance.

After some data exploration, we can finally develop (train) and evaluate (test) predictive models. In other words, we are going to find the best parameters for the different models and test them. Can we train and test models using exactly the same data? That would not be a good idea at all, as it would results in an overestimation of the accuracy of our models (we would end it up fitting also noise). Why? 

Imagine to be muay Thai fighter that is getting ready for his first fight. If you train and spar always with the same person, you might end it up knowing his/her strategies pretty well, and developing way to respond and exploit his/her way of attacking and defending. For an external observer you might look even very good when you spar with him/her. Does it mean that you are going to do well also in your fight? Not really. Your opponent might have a difference stance of your training partner or a completely different style. You might have not encountered those moves in training and thus perform not well at all under those different conditions (and get your ass kicked). So what can you do to get ready for your fight and have a more accurate understanding of your potential? Don't train and test your skill always with the same person. What you want to do is to train with as many different people, and then occasionally test your skills with someone that you have never trained before. This will maximize your chance of having trained for someone similar to your actual fight opponent but also give you a better idea if you are ready or not.

That is the same thing that we are going to do with our data. We will divide our dataset in 10 samples, train our model with 9 of them and evaluate it with the 10th. Then select a different sample for testing and apply the same procedure until each one is used once for testing. In this way, we well never train and test our model with the same data and we can also get an estimate of variability in the accuracy of the model. The entire procedure will than be repeated 5 times with reshuffled data (new partitions). This procedure is called ```repeated cross-validation```.

Unfortunately we have another problem. The people that keep taking drugs despite negative consequences are about 5% of the total number of people in our dataset. We have a severe **class imbalance**. Because of such a difference in the size of our 2 classes, it is more likely that the small class we will have much less weight in the parameter optimization of our model. There are several things that we can do to mitigate that problem. One is to randomly up-sample the smaller class to make the class distribution equal. We could run the caret function ```upSample``` that does exactly that and than train and test our model on the re-sampled data.  What is the problem in doing that? It's that it is very likely that the same observations will be part of the data that used to train the model but also test its accuracy, that is because  we are going to duplicate a lot of observations for the class "yes". Here we are again, training and *sparring* with the same partners as before! We need to train our model on the up Sample data, but then test it on part of the not re-sampled data! [_*Max Kuhn*_](https://topepo.github.io/caret/index.html), the creator of ```caret```, made it easy for us, and not we can do that using an option when we invoke the function ```trainControl``` for cross-validating the data. Perfect!

## TrainControl and Hyperparameters

There is another important detail to take in consideration when we partition data.  We want to make sure to maintain as equal as possible the distribution of our class of interest in the train and test samples. In other words, we want to avoid that for example the "*yes*" responses to question *H4TO117* will end up all in one of the training partitions but not the other. This is done automatically by ```caret::trainControl```.

However, our dataset has another relevant variable that we need to take in consideration: *CLUSTER2*. In the ADD HEALTH study, subjects were chosen in clusters (i.e. schools). It is likely that observations of same clusters will be more similar to each other than to those coming from a different cluster. Again, if the data that we use for training and testing the model are very similar, we might end up obtain overoptimistic estimates [overoptimistc estimates](https://topepo.github.io/caret/data-splitting.html#simple-splitting-with-important-groups) of accuracy. This would be equivalent to test your fighting skills sparring with someone that you have never trained with before but that comes to your same gym. He/she has trained under the same coach and thus it is very likely that is going to employ moves that you have seen before. Would you still perform well against someone with a completely different style and background? Difficult to say. It would be better to test your skills sparring with someone from a different gym. Similarly, we can limit this problem taking in account the variable ```CLUSTER2``` when we partition our data  and try to avoid as much as possible to train and test the model with data from exactly the same cluster. The function ```caret::groupKFold``` will do it for us.

```{r trainControl}
seeds = 1502

heal_folds <- groupKFold(heal_sub$CLUSTER2, k = 10)

fitControl <- trainControl(method = "adaptive_cv",
                           adaptive = list(min = 2,  # the minimum number of resamples used before models are removed
                                           alpha = 0.05, # confidence intervals for futility
                                           method = "gls", #  generalized least squares
                                           complete = TRUE), # calcolate all resamples even if value is found
                           repeats = 5,
                           number = 10,
                           index = heal_folds,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary,
                           savePredictions = "final",
                           sampling = "up",   # upsampling
                           verboseIter = TRUE)
```


```{r trainControl2, eval=FALSE, include=FALSE}
seeds = 1502

heal_folds <- groupKFold(heal_sub$CLUSTER2, k = 10)

fitControl2 <- trainControl(method = "adaptive_cv",
                           adaptive = list(min = 2,  # the minimum number of resamples used before models are removed
                                           alpha = 0.05, # confidence intervals for futility
                                           method = "gls", #  generalized least squares
                                           complete = TRUE), # calcolate all resamples even if value is found
                           repeats = 5,
                           number = 10,
                           index = heal_folds,
                           classProbs = TRUE,
                           summaryFunction = prSummary,
                           savePredictions = "final",
                           sampling = "up",   # upsampling
                           verboseIter = TRUE)



mod_rpart_pr <- train(H4TO117 ~ ., 
              heal_sub_cl,
              method = "rpart",
              tuneLength = 5,
              metric = "F",
              # weights = heal_weights,
              trControl = fitControl2,
              na.action = na.pass,
              preProcess = c("nzv", "corr","knnImpute","pca")
              )


mod_ctree_pr <- train(H4TO117 ~ ., 
              heal_sub_cl,
              method = "ctree",
              tuneLength = 5,
              metric = "F",
              # weights = heal_weights,
              trControl = fitControl2,
              na.action = na.pass,
              preProcess = c("nzv", "corr","knnImpute","pca")
              )
```

All the operations that we discussed in the last two paragraph are taken care of by ```caret::trainControl```. You might have notice that in the code above we are not using a *repeated cross-validation`* but we have indicated instead ```method = "adaptive_cv```.  What is that? It is an [algorithm](https://topepo.github.io/caret/adaptive-resampling.html) to  adaptability look for the best *tuning parameters* for our model and, as Max Kuhn put it "concentrates on values that are the in the neighborhood of the optimal settings".
Let's clarify what are this *tuning parameters*. The tuning parameters (hyperparamters) are those parameters that are not derived by the data (like the coefficients of a regression) but are decided beforehand. For example, in order to increase the interpretability of regression and reduce co-linearity, we might decide to limit the number of coefficients and/or their total magnitude and, thus develop a more parsimonious model. In the package ```glmnet``` we can do that setting the hyperparameters ```alpha``` and ```lambda```.  Similarly, in decision tree we need to decide how many variables to consider at each split (```mtry```) or other parameters that control the tree size (```cp```). Using ```method = "adaptive_cv```, combinations of hyperparmetes will be automatically searched and evaluated in each sample to obtain the best parameters _*and hyperparameters*_ for the model (i.e. that maximize accuracy).

## Preprocessing 

Before training our first model we need to spend also some few words on preprocessing. This is getting long but no one will really read this post so who cares. We have a lot of variables, more than 2000 and that put quite some computational burden on my personal laptop. We might no need all those variables, because some of them might have near zero variance (imagine a column made of only one identical value) or might be very correlated to each other. Moreover, we can using principal component analysis ([PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)) to use linear combination of variables that explain most of the variability of the data. Finaly, we can impute missing data using k-nearest neighbors algorithms, that use the distance to the closest sample to calculate the values that are substitute to the missing data. All this computations can be very conveniently done before training our models using the argument ```preProcess = c("nzv", "corr","knnImpute","pca")```, when calling the function ```caret::train```.

## GLMNET

We are ready! Let's train our first model, ["a generalized linear model via penalized maximum likelihood"](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html), using the package above mentioned ```glmnet```. Because of the limited computational capability of my laptop we are going to try only 5 different combinations of hyperparameters for each resample (````tuneLength = 5```). _*This is definitely an inadequate number of combinations*_ (I would have tried at least 100 if I could) _*but will still give as the opportunity to see how to train a model an evaluate it.*_ 

```{r mod_glmnet, eval=FALSE, include=TRUE}
mod_glmnet <- train(H4TO117 ~ ., 
              heal_sub_cl,
              method = "glmnet",
              tuneLength = 5,
              # weights = heal_weights,
              trControl = fitControl,
              na.action = na.pass,
              preProcess = c("nzv", "corr","knnImpute","pca")
              )
```

All the info regarding our model, including the coefficients of the different predictors, are contained in ```mod_glmnet$finalModel```. A summary of the details regarding the *pre-processing*, *resampling*, *hyperparmeter tuning* can be easily retrived ```print``ing  the model.

```{r mod_print, include=FALSE}
confusionMatrix(mod_glmnet)
```
Notice that many variables, more than the number of columns, have been removed during the ```pre-Processing```. This is because ```caret``` automatically convert factors in [*dummy variables*](https://topepo.github.io/caret/pre-processing.html#creating-dummy-variables).

_*Was the model was the model any good? Was it able to predict if a person was going to take the favorite drug despite negative consequences?*_.

Some estimates of the model efficacy are reported above (ROC, Sens, Spec) and we will discuss them in depth during the next paragraphs. Right now, we can get a feel of how the model is performing by looking at the [confusionMatrix](https://en.wikipedia.org/wiki/Confusion_matrix) which indicates the correct prediction in each of the classes. Instead of using the ```caret::confusionMatrix```, I am going to create my own function for extracting that info from ```model$table``` and arrange it in table with a different layout.

```{r eval_glmnet_matrix}
confusionTable <- function (mod) {
  tidy(confusionMatrix(mod)$table) %>% 
  mutate( model = mod$method) %>% 
  spread(model, n) %>% 
  mutate(Case = c("True Negative", 
                   "False Negative",
                   "False Postive", 
                   "True Positive")) %>% 
  arrange(Case) %>% 
  select(Case, everything()) %>% 
  mutate_if(is.numeric, round, 2)
}

table_glmnet <- confusionTable(mod_glmnet)
table_glmnet
```
You can see that while the model doesn't do that bad in predicting *No*, the performance deteriorate in identifying *Yes*. About 5% of the subjects in our dataset respond *Yes* with our model detecting less than half of them (true positive).  Let's try with different models.

## Classification Trees and Random Forest

Another type of very popular predictive model is classification tree. Classification trees are very easy to implement and interpret (if number of predictors are not too much) and consist in recursively spliting the data in groups that are internally as homogeneus as possible in terms of measures of purity (i.e Gini index) of the target classifier. Here a good visual example of possible [splits](https://www.google.com/url?sa=i&source=images&cd=&ved=2ahUKEwjx4YPhpOngAhXMhOAKHeykDx0QjRx6BAgBEAU&url=https%3A%2F%2Fdimensionless.in%2Fintroduction-to-random-forest%2F&psig=AOvVaw2kexYImpv-x0wyxpnd_mzC&ust=1551816401888411) and of their relative level of *purity*. In other words, what the tree does is to create a concatatenation of if-then. 
Continouing the analogy with Muay Thai, a branch of a classification tree might look like this : _*if*_ fighter A trains 7 days a week _*if*_ his/her opponent train only once a week and _*if*_ fighter A has 150 fights more than his opponent and _*if*_ fighter A is less than 30 years old and more than 18 _*than*_ fighter A is going to win the fight.

We  grow the first tree using the package ```rpart```.

```{r mod_rpart, eval=FALSE, include=TRUE}
mod_rpart <- train(H4TO117 ~ ., 
              heal_sub_cl,
              method = "rpart",
              tuneLength = 5,
              # weights = heal_weights,
              trControl = fitControl,
              na.action = na.pass,
              preProcess = c("nzv","corr", "knnImpute","pca")
              )
```

Let's grow also a second tree using the package ```party```. The major difference with the preceding classification tree model is that ```party::ctree``` implement  infernatial static to evaluate improvements in measures of purity and thus to guide the recursive splitting.

```{r mod_ctree, eval=FALSE, include=TRUE}
mod_ctree <- train(H4TO117 ~ ., 
                   heal_sub_cl,
                   method = "ctree",
                   metric = "ROC",
                   tuneLength = 5,
                   trControl = fitControl,
                   na.action = na.pass,
                   preProcess = c("nzv","corr", "knnImpute","pca")
              )
```

Finaly, we will train and evaluate a Random Forest model, that is an essamble of multiple classification trees. Usually Random Forest perform better than classification trees but this increase  accuracy comes with increase model complexity and decrease interpreatability.

```{r mod_rf, eval=FALSE, include=TRUE}
mod_ranger <- train(H4TO117 ~ ., 
              heal_sub_cl,
              met2hod = "ranger",
              metric = "ROC",
              tuneLength = 5,
              trControl = fitControl,
              na.action = na.pass,
              preProcess = c("nzv","corr", "knnImpute", "pca")
              )
```

How did the models performed? We will look in dept at different measures of accuracy in the next paragraph.

# Evaluation of the models

## Confusion Matrix

Firstofall, we create a confusion "table' with the results of all 4 models.

```{r eval_matrix, eval=TRUE, include=FALSE}
table_m4  <-  bind_cols(confusionTable(mod_glmnet),
                confusionTable(mod_rpart)[,4],
                confusionTable(mod_ctree)[,4],
                confusionTable(mod_ranger)[,4])
table_m4 
```

Of the 4 models, the ```glmnet``` is the one that has the highest rate of detection of *Yes* (True Positive), but also the highest rate of False Allarms (False Positive). The 2 tree based models detect only a very small fraction of True Positive, whereas the Random Forest does not get any hit and always bets for *No*.

## ROC curves

Another way to evaluate and compare performance of different models in more detail is to look at the ROC curves. In ROC curves, the cutoff probabilty for assigning the prediction values ("yes" or "no") is varied systematically, and for each cutoff value the False Alarms generated by the model are plotted as a function of True Positives.

We create a list of our models and the function ```get_roc``` to extract the data for the ROC curve from each of them.

```{r ROC_function}
model_list <- list( glmnet = mod_glmnet,
                    rpart = mod_rpart,
                    ctree = mod_ctree,
                    rf = mod_ranger)

get_roc <- function(mod, cl) {
   # function to extract ROC data from the different models
   # Returns a dataframe 
   # mod = model
   # cl = character, name of class of interest 
  
   if (missing(cl)) stop("You have not inserted the class of interest!")
   if (!cl %in% names(mod[["pred"]])) stop("Class of interest not present!")
  
   for_lift <-  tibble(Class = mod[["pred"]][["obs"]],  model = mod[["pred"]][[cl]])
   lift_obj  <-  lift(Class ~ model, data = for_lift, class = cl)
   lift_obj$data$liftModelVar <- mod[["method"]]
   lift_obj$data
}
```

We apply the function to the list of models, we reshape the dataframe and plot the ROC with ```ggplot```. 

```{r ROC_plot, fig.align="center"}
roc_data <- map_dfr(model_list, get_roc, "Yes")

roc_data <-roc_data %>% 
  # clean up and get important variables
  rename(model = liftModelVar) %>% 
  mutate(model = factor(model, levels = names(table_m4[4:7]))) %>%  # let's be consistent with the order 
  mutate(TPR = Sn, FPR = 1 - Sp) 
  
roc_data %>% 
  # plot
  ggplot(aes(y = TPR, x = FPR, col = model)) +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = 2) +
  # facet_wrap(~model, ncol = 2) +
  labs( y =  "True Positive Rate (TP/P)",
        x = "False Positive Rate (1-TN/N)")
```

A perfect model will a curve that goes through the upper left corner and thus have specificity and selectivity equal to 1. A model with that gives random predictions will generate a curve that will lie in vicinity the dotted line. Of course we might have also a model that make predictions worse than random, in that case the curve will be to the right of the dotted line.

A near-perfect diagnostic test would have an ROC curve that is almost
vertical from (0,0) to (0,1) and then horizontal to (1,1). The diagonal line serves as a reference line since it is the
ROC curve of a diagnostic test that randomly classifies the condition.

Sensitivity o Recal = TP / P

Specifivity = TN / N

Precision = TP / (TP + FP)


## Sensitivity, Specificity and Balanced Accuracy

```{r all_graph, eval=TRUE, include=TRUE, warning=FALSE}
model_list <- list( glmnet = mod_glmnet,
                    rpart = mod_rpart,
                    ctree = mod_ctree,
                    rf = mod_ranger)

model_list <- list( rpart = mod_rpart_pr,
                    ctree = mod_ctree_pr)



resamps <- resamples(model_list)

resamps$values %>% 
  gather("model~metric", "value", 2:13) %>%  # from wide to long format
  separate ("model~metric", c("model", "metric")) %>%  # separate columns
  mutate(model = factor(model, levels = names(table_m4[4:7]))) %>% # we reorder the levels of model
  spread(metric, value) %>% # long to wide so that we can calculate balanced accuracy
  mutate("Balanced Accuracy" = (Sens + Spec)/ 2) %>%  # we calculate the balance accuracy
  rename(Sensitivity = Sens, Specificity = Spec) %>% 
  gather("metric",  "value", 3:6) %>% 
  # Calculate mean
  group_by(model, metric) %>% 
  summarize(mean = mean(value), sd = sd(value)) %>% 
  mutate(metric = factor(metric, levels = c("ROC", "Sensitivity", "Specificity", "Balanced Accuracy"))) %>% 
  
  
  ggplot(aes(y= mean, x = model, col = model)) +
    geom_point(size = 2.5) +
    geom_errorbar(aes(ymin = mean - sd, ymax =  mean + sd), width = 0.1) +
    scale_y_continuous(limits = c(0, 1)) + 
    coord_flip() +
    facet_wrap(metric ~ .) +
    theme(legend.position = "none")
```

## Precision-Recall Curves

```{r precision_recal}

get_pr <- function(mod) {
   pr_data <- pr.curve(mod[["pred"]][["pred"]], mod[["pred"]][["obs"]], curve = TRUE) 
   pr_data <-  as.data.frame(pr_data$curve[,1:2]) %>% 
   rename( "Precision" = V2, "Recall" = V1) %>% 
   mutate(model = mod[["method"]])
}  
   
   
pr_data <- map_dfr(model_list, get_pr)

pr_data %>% 
  mutate(model = factor(model, levels = names(table_m4[4:7]))) %>%  # let's be consistent with the order 
  ggplot(aes(y = Precision, x = Recall, col = model)) +
  geom_line() +
  # facet_wrap(~model, ncol = 2) +
  labs( y =  "Precision TP/(TP+FP)",
        x = "Recall (TP/P)",
        title = "Precision-Recall curve")

```

```{r}
plot(prova)
```
## GAIN CURVE



```{r}


twoClassSummary(prova, lev = c("Yes", "No"))



```


