---
title: "Cassandra, Aceso and the trees"
output: html_notebook
---

In this post we will apply predicting modelling to data of [__ADD HEALTH__](https://www .cpc.unc.edu/projects/addhealth/about), a national longitudinal study that enrolled  ~90000 adolescents and followed them up for 20 years. Using the *R* package [```caret```](https://github.com/topepo/caret), we will train and evaluate different models (linear, decison tree, random Forrest) in order to predict drug use behavior in adulthood from information (friends, social activities, parents and others) collected during adolescence. Doing that will give us the opportunity to play and get familiarized with the most important ```caret``` functions as well as introduce seminal concepts of machine learning.

# The Data

__ADD HEALTH__ is a unique study both in terms of the extent of information collected ("the largest, most comprehensive longitudinal survey of adolescents ever undertaken") and duration (started in 1994 and is still on going [study design](https://www.cpc.unc.edu/projects/addhealth/design/designpage001.jpg/@@images/b842a2e4-e345-4384-bfc5-c724a3b55b5c.jpeg)).

We will analyze the  [publicly available data](https://www.cpc.unc.edu/projects/addhealth/documentation/publicdata), a small fraction of the total subjects surveyed ~5%, but still a quite decent dataset of ~ 6000 observations X several thousand variables.

It is important to keep in mind that the __current analysis will be performed on unweighted data__ (ADD  sampling design is quite complex) and models will be evaluated in samples not necessaries representative of the population. __This can affect the final estimates of the model effectiveness__. 

Users interested in the specific statistical procedures suggested for ADD Health weight adjustment can find info in this [guide](https://www.cpc.unc.edu/projects/addhealth/documentation/guides/wt_guidelines_20161213.pdf) and this [article](https://www.jstor.org/stable/2985969?seq=1#page_scan_tab_contents).

# Obtain the dataset and make it tidy

We start loading some libraries.
```{r libraries, message=FALSE}
library(tidyverse)
library(purrr)
library(foreign) 
library(svDialogs)
library(caret)
library(glmnet)
library(randomForest)
library(broom)
library(rpart)
library(party)
library(MLmetrics)
library(PRROC)
library(precrec)
library(rlang)
library(forcats)
library(grid)
```


```{r load_all, eval= TRUE, include=FALSE}
path_m <- "/Users/heverz/Documents/R_projects/muaydata/static/data_post4/"
mod_glmnet <- readRDS(paste0(path_m,"mod_glmnet_ROC.RDS"))
mod_rpart <- readRDS(paste0(path_m,"mod_rpart_ROC.RDS"))
mod_ctree <- readRDS(paste0(path_m,"mod_ctree_ROC.RDS"))
mod_ranger <- readRDS(paste0(path_m,"mod_ranger_ROC.RDS"))

mod_glmnet_F <- readRDS(paste0(path_m,"mod_glmnet_F.RDS")) 
mod_glmnet_AUC  <- readRDS(paste0(path_m,"mod_glmnet_AUC.RDS")) 

model_list <- list( glmnet = mod_glmnet,
                    rpart = mod_rpart,
                    ctree = mod_ctree,
                    ranger = mod_ranger)


confusionTable <- function(mod) {
          UseMethod("confusionTable", mod)
}


confusionTable.default <- function (mod) {
    if (!is(mod, "train")) stop("The object selected is not a model")
  
    tidy(confusionMatrix(mod)$table) %>% 
    mutate(modname = mod[["method"]]) %>% 
    spread(modname, n) %>% 
    mutate(Case = c("True Negative", 
                   "False Negative",
                   "False Postive", 
                   "True Positive")) %>% 
    arrange(Case) %>% 
    select(Case, everything()) %>% 
    mutate_if(is.numeric, round, 2)
}

confusionTable.list <- function(mod) {
    map_dfc(mod, confusionTable) %>% 
    select(1:4, seq(8, length(mod)*4, 4))
}


mod_table <-  confusionTable(model_list)
mod_names<- names(mod_table)[4 : ncol(mod_table)]

```

The number of variables collected for the ADD HEALTH is of several thousands. We will focus our analysis on most of the variables collected in the first wave (first survey in 1994) that include demographic information, friend and social network details, parental information and many others) and we will select one classifier from the fourth wave (2006). 

A description of all the variables of the study can be found using the [codebook web Explorer](https://www.cpc.unc.edu/projects/addhealth/documentation/ace/tool/topics). 

I downloaded the public-use data from [ICPSR](https://www.icpsr.umich.edu/icpsrweb/DSDR/studies/21600#) and then saved all the files with *dta* (Stata) extension in a folder. At this point, we can import all the files, select the columns that are in var_select and join the dataframes together by the subject identification variable (```AID``` that then we remove). Finaly, all the the columns that contain character data are transformed to factors.

```{r getdata}
files <- list.files(path = "/Users/heverz/Documents/R_projects/DATASETS/W1-4",
                    include.dirs=FALSE,
                    full.names = TRUE)

# import all the dataframes.
w1_4 <- map(files, read.dta)

# identify the name of varibles of interest
h1_pa <- names(w1_4[[1]])[str_detect(names(w1_4[[1]]), '^H1|PA')] # H1 is the first wave 
ids <- c("AID", "GSWGT4_2", "CLUSTER2", "H4TO117") # H4 is the classifier and GSW are the weights
var_select <- append(h1_pa, ids)

# select the variables
w1_4_reduced <- map(w1_4, function (x) x[, colnames(x) %in% var_select])

# join not empty dataframes
toselect <- which(map(w1_4_reduced, function(x) ncol(x) > 0) == TRUE) # list of not empty dataframes
heal_data <- plyr::join_all(w1_4_reduced[toselect] , by = "AID", type = "inner")

# two colums same name /remove one
heal_data <- heal_data[, -2337] 
names(heal_data)[2335] <- "CLUSTER2"

# character columns are factors
heal_sub <- mutate_if(heal_data, is.character, as.factor)
```

Some columns are factors with only one level. We are going to remove them.

```{r remove_1factor}
# some columns have only one level
nlevel_count <- map_int(heal_sub, nlevels)
lev_1 <- unlist(attributes(nlevel_count[nlevel_count == 1]))

heal_sub <- heal_sub[,  !names(heal_sub) %in% lev_1]
```

Same columns miss quite a lot of data. We can *impute* missing observation only if there are not too many in each column. We are going to drop all the columns that have more than 15% of missing data.

```{r NAs}
na_byc <-  unlist(map(heal_sub, function(x) sum(is.na(x))))

# if more than 15% NA in column then not take the column
to_remove <- na_byc[na_byc >= nrow(heal_sub)*0.15]  

# subset
heal_sub <- heal_sub[,!names(heal_sub) %in% unlist(attributes(to_remove))]
```

The _*H4TO117*_ column is our classifier, the response to the following question:

*"Have you ever continued to use {favorite drug} after you realized using {favorite drug} was causing you any emotional problems (such as feeling depressed or empty, feeling irritable or aggressive, feeling paranoid or confused, feeling anxious or tense, being jumpy or easily startled) or causing you any health problems (such as heart pounding, headaches or dizziness, or sexual difficulties)?"*

Let's clean it up and see the responses.

```{r select_classifier}
#classifier
heal_sub_cl <- 
  heal_sub %>%
  select(-c("AID","GSWGT4_2", "CLUSTER2", "H1GI1M")) %>%  # remove column of weight, month of birth, and clusters 
  mutate(H4TO117 = as.factor(str_extract(H4TO117 , "[:alpha:]+")))  # keep only letters

```

*Legitimate* stands for Legitimate skips, that indicates those people that do not have a favorite drug.

The answers to this question not only allow to distinguish people that use (or had used drugs) from people that did not (legitimate skip), but also to identify those individuals that continue to take drugs despite negative consequences (yes). The class "yes" will be our class of interest because it indicates those people that are presumably more likely to develop health problems as a consequence of drug use patterns. 

Therefore, we will try first to  fit 2 class models, before trying something more complex. We replace "Legitimate" with "No" to obtain 2 classes: those that continued to take drugs despite negative consequences (*yes*) and those that did not (*no*).

```{r 2Classes}
heal_sub_cl$H4TO117[heal_sub_cl$H4TO117 == "Legitimate"] <- "No"

heal_sub_cl$H4TO117 <- droplevels(heal_sub_cl$H4TO117)
 
heal_sub_cl$H4TO117 <- factor(heal_sub_cl$H4TO117, levels = c("Yes", "No")) # class of interest need to  be the first

summary(heal_sub_cl$H4TO117)
```

#   Train the Models

## Cross Validation, Class Inbalance.

After some data exploration, we can finally develop (train) and evaluate (test) predictive models. In other words, we are going to find the best parameters for the different models and test them. Can we train and test models using exactly the same data? That would not be a good idea at all, as it would results in an overestimation of the accuracy of our models (we would end it up fitting also noise). Why? 

Imagine to be muay Thai fighter that is getting ready for his first fight. If you train and spar always with the same person, you might end it up knowing his/her strategies pretty well, and developing way to respond and exploit his/her way of attacking and defending. For an external observer you might look even very good when you spar with him/her. Does it mean that you are going to do well also in your fight? Not really. Your opponent might have a difference stance of your training partner or a completely different style. You might have not encountered those moves in training and thus perform not well at all under those different conditions (and get your ass kicked). So what can you do to get ready for your fight and have a more accurate understanding of your potential? Don't train and test your skill always with the same person. What you want to do is to train with as many different people, and then occasionally test your skills with someone that you have never trained before. This will maximize your chance of having trained for someone similar to your actual fight opponent but also give you a better idea if you are ready or not.

That is the same thing that we are going to do with our data. We will divide our dataset in 10 samples, train our model with 9 of them and evaluate it with the 10th. Then select a different sample for testing and apply the same procedure until each one is used once for testing. In this way, we well never train and test our model with the same data and we can also get an estimate of variability in the accuracy of the model. The entire procedure will than be repeated 5 times with reshuffled data (new partitions). This procedure is called ```repeated cross-validation```.

Unfortunately we have another problem. The people that keep taking drugs despite negative consequences are about 5% of the total number of people in our dataset. We have a severe **class imbalance**. Because of such a difference in the size of our 2 classes, it is more likely that the small class we will have much less weight in the parameter optimization of our model. There are several things that we can do to mitigate that problem. One is to randomly up-sample the smaller class to make the class distribution equal. We could run the caret function ```upSample``` that does exactly that and than train and test our model on the re-sampled data.  What is the problem in doing that? It's that it is very likely that the same observations will be part of the data that used to train the model but also test its accuracy, that is because  we are going to duplicate a lot of observations for the class "yes". Here we are again, training and *sparring* with the same partners as before! We need to train our model on the up Sample data, but then test it on part of the not re-sampled data! [_*Max Kuhn*_](https://topepo.github.io/caret/index.html), the creator of ```caret```, made it easy for us, and not we can do that using an option when we invoke the function ```trainControl``` for cross-validating the data. Perfect!

## TrainControl and Hyperparameters

There is another important detail to take in consideration when we partition data.  We want to make sure to maintain as equal as possible the distribution of our class of interest in the train and test samples. In other words, we want to avoid that for example the "*yes*" responses to question *H4TO117* will end up all in one of the training partitions but not the other. This is done automatically by ```caret::trainControl```.

However, our dataset has another relevant variable that we need to take in consideration: *CLUSTER2*. In the ADD HEALTH study, subjects were chosen in clusters (i.e. schools). It is likely that observations of same clusters will be more similar to each other than to those coming from a different cluster. Again, if the data that we use for training and testing the model are very similar, we might end up obtain overoptimistic estimates [overoptimistc estimates](https://topepo.github.io/caret/data-splitting.html#simple-splitting-with-important-groups) of accuracy. This would be equivalent to test your fighting skills sparring with someone that you have never trained with before but that comes to your same gym. He/she has trained under the same coach and thus it is very likely that is going to employ moves that you have seen before. Would you still perform well against someone with a completely different style and background? Difficult to say. It would be better to test your skills sparring with someone from a different gym. Similarly, we can limit this problem taking in account the variable ```CLUSTER2``` when we partition our data  and try to avoid as much as possible to train and test the model with data from exactly the same cluster. The function ```caret::groupKFold``` will do it for us.

```{r trainControl}
seeds = 1502

heal_folds <- groupKFold(heal_sub$CLUSTER2, k = 10)

fitControl <- trainControl(method = "adaptive_cv",
                           adaptive = list(min = 2,  # the minimum number of resamples used before models are removed
                                           alpha = 0.05, # confidence intervals for futility
                                           method = "gls", #  generalized least squares
                                           complete = TRUE), # calcolate all resamples even if value is found
                           repeats = 5,
                           number = 10,
                           index = heal_folds,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary,
                           savePredictions = "final",
                           sampling = "up",   # upsampling
                           verboseIter = TRUE)
```

```{r trainControl2, eval=FALSE, include=FALSE}
seeds = 1502

heal_folds <- groupKFold(heal_sub$CLUSTER2, k = 10)

fitControl2 <- trainControl(method = "adaptive_cv",
                           adaptive = list(min = 2,  # the minimum number of resamples used before models are removed
                                           alpha = 0.05, # confidence intervals for futility
                                           method = "gls", #  generalized least squares
                                           complete = TRUE), # calcolate all resamples even if value is found
                           repeats = 5,
                           number = 10,
                           index = heal_folds,
                           classProbs = TRUE,
                           summaryFunction = prSummary,
                           savePredictions = "final",
                           sampling = "up",   # upsampling
                           verboseIter = TRUE)

mod_glmnet_pr <- train(H4TO117 ~ ., 
              heal_sub_cl,
              method = "glmnet",
              tuneLength = 5,
              metric = "F",
              # weights = heal_weights,
              trControl = fitControl2,
              na.action = na.pass,
              preProcess = c("nzv", "corr","knnImpute","pca")
              )

saveRDS(mod_glmnet_pr, "mod_glmnet_pr.RDS")


mod_ctree_pr <- train(H4TO117 ~ ., 
              heal_sub_cl,
              method = "ctree",
              tuneLength = 5,
              metric = "F",
              # weights = heal_weights,
              trControl = fitControl2,
              na.action = na.pass,
              preProcess = c("nzv", "corr","knnImpute","pca")
              )



mod_rpart_pr <- train(H4TO117 ~ ., 
              heal_sub_cl,
              method = "rpart",
              tuneLength = 5,
              metric = "F",
              # weights = heal_weights,
              trControl = fitControl2,
              na.action = na.pass,
              preProcess = c("nzv", "corr","knnImpute","pca")
              )


mod_ctree_pr <- train(H4TO117 ~ ., 
              heal_sub_cl,
              method = "ctree",
              tuneLength = 5,
              metric = "F",
              # weights = heal_weights,
              trControl = fitControl2,
              na.action = na.pass,
              preProcess = c("nzv", "corr","knnImpute","pca")
              )


```

All the operations that we discussed in the last two paragraph are taken care of by ```caret::trainControl```. You might have notice that in the code above we are not using a *repeated cross-validation`* but we have indicated instead ```method = "adaptive_cv```.  What is that? It is an [algorithm](https://topepo.github.io/caret/adaptive-resampling.html) to  adaptability look for the best *tuning parameters* for our model and, as Max Kuhn put it "concentrates on values that are the in the neighborhood of the optimal settings".
Let's clarify what are this *tuning parameters*. The tuning parameters (hyperparamters) are those parameters that are not derived by the data (like the coefficients of a regression) but are decided beforehand. For example, in order to increase the interpretability of regression and reduce co-linearity, we might decide to limit the number of coefficients and/or their total magnitude and, thus develop a more parsimonious model. In the package ```glmnet``` we can do that setting the hyperparameters ```alpha``` and ```lambda```.  Similarly, in decision tree we need to decide how many variables to consider at each split (```mtry```) or other parameters that control the tree size (```cp```). Using ```method = "adaptive_cv```, combinations of hyperparmetes will be automatically searched and evaluated in each sample to obtain the best parameters _*and hyperparameters*_ for the model (i.e. that maximize accuracy).

## Preprocessing 

Before training our first model we need to spend also some few words on preprocessing. This is getting long but who cares? no one will really read this post (maybe Alessandro will check it out). We have a lot of variables, more than 2000 and that put quite some computational burden on my personal laptop. We might no need all those variables, because some of them might have near zero variance (imagine a column made of only one identical value) or might be very correlated to each other. Moreover, we can using principal component analysis ([PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)) to use linear combination of variables that explain most of the variability of the data. Finaly, we can impute missing data using k-nearest neighbors algorithms, that use the distance to the closest sample to calculate the values that are substitute to the missing data. All this computations can be very conveniently done before training our models using the argument ```preProcess = c("nzv", "corr","knnImpute","pca")```, when calling the function ```caret::train```.

## GLMNET

We are ready! Let's train our first model, ["a generalized linear model via penalized maximum likelihood"](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html), using the package above mentioned ```glmnet```. Because of the limited computational capability of my laptop we are going to try only 5 different combinations of hyperparameters for each resample (````tuneLength = 5```). _*This is definitely an inadequate number of combinations*_ (I would have tried at least 100 if I could) _*but will still give as the opportunity to see how to train a model an evaluate it.*_ 

```{r mod_glmnet, eval=FALSE, include=TRUE}
mod_glmnet <- train(H4TO117 ~ ., 
              heal_sub_cl,
              method = "glmnet",
              tuneLength = 5,
              # weights = heal_weights,
              trControl = fitControl,
              na.action = na.pass,
              preProcess = c("nzv", "corr","knnImpute","pca")
              )
```

All the info regarding our model, including the coefficients of the different predictors, are contained in ```mod_glmnet$finalModel```. A summary of the details regarding the *pre-processing*, *resampling*, *hyperparmeter tuning* can be easily retrived ```print``ing  the model.

```{r mod_print, include=FALSE}
print(mod_glmnet)
```
Notice that many variables, more than the number of columns, have been removed during the ```pre-Processing```. This is because ```caret``` automatically convert factors in [*dummy variables*](https://topepo.github.io/caret/pre-processing.html#creating-dummy-variables).

_*Was the model was the model any good? Was it able to predict if a person was going to take the favorite drug despite negative consequences?*_.

Some estimates of the model efficacy are reported above and we will discuss them in depth later in the post. Now, let's train few other models before looking at them all together.

## Classification Trees and Random Forest

Another type of very popular predictive model is classification tree. Classification trees are very easy to implement and interpret (if number of predictors are not too much) and consist in recursively spliting the data in groups that are internally as homogeneus as possible in terms of measures of purity (i.e Gini index) of the target classifier. Here a good visual example of possible [splits](https://www.google.com/url?sa=i&source=images&cd=&ved=2ahUKEwjx4YPhpOngAhXMhOAKHeykDx0QjRx6BAgBEAU&url=https%3A%2F%2Fdimensionless.in%2Fintroduction-to-random-forest%2F&psig=AOvVaw2kexYImpv-x0wyxpnd_mzC&ust=1551816401888411) and of their relative level of *purity*. In other words, what the tree does is to create a concatatenation of if-then. 
Continouing the analogy of Muay Thai, a branch of a classification tree might look like this : _*if*_ fighter A trains 7 days a week _*if*_ his/her opponent train only once a week and _*if*_ fighter A has 150 fights more than his opponent and _*if*_ fighter A is less than 30 years old and more than 18 _*than*_ fighter A is going to win the fight.

We  grow the first tree using the package ```rpart```.

```{r mod_rpart, eval=FALSE, include=TRUE}
mod_rpart <- train(H4TO117 ~ ., 
              heal_sub_cl,
              method = "rpart",
              tuneLength = 5,
              # weights = heal_weights,
              trControl = fitControl,
              na.action = na.pass,
              preProcess = c("nzv","corr", "knnImpute","pca")
              )
```

Let's grow also a second tree  model using the package ```party```. The major difference with the preceding classification tree model is that ```party::ctree``` implement  inferential statistic to evaluate improvements in measures of purity, and thus to guide the recursive splitting.

```{r mod_ctree, eval=FALSE, include=TRUE}
mod_ctree <- train(H4TO117 ~ ., 
                   heal_sub_cl,
                   method = "ctree",
                   metric = "ROC",
                   tuneLength = 5,
                   trControl = fitControl,
                   na.action = na.pass,
                   preProcess = c("nzv","corr", "knnImpute","pca")
              )
```

Finaly, we will train and evaluate a Random Forest model, that is an essamble of multiple classification trees. Usually Random Forest perform better than classification trees but this increased accuracy comes at the expenses of the model complexity and interpreatability.

```{r mod_rf, eval=FALSE, include=TRUE}
mod_ranger_pr <- train(H4TO117 ~ ., 
              heal_sub_cl,
              met2hod = "ranger",
              metric = "F",
              tuneLength = 5,
              trControl = fitControl2,
              na.action = na.pass,
              preProcess = c("nzv","corr", "knnImpute", "pca")
              )
```

How did the models performed? We will look in dept at different measures of accuracy in the next paragraph.

# Evaluation of the models

## Confusion Matrix

We can get a first feel of how the model is performing by looking at the [confusionMatrix](https://en.wikipedia.org/wiki/Confusion_matrix) which indicates the correct prediction in each of the classes. Instead of using the ```caret::confusionMatrix```, I am going to create my own function for extracting that info from ```model$table``` and arrange it in table with a different layout.
We will create 2 different method dispatches depending on the class of the object in order to easily apply the function to a single model or a list of models. 

```{r eval_matrix, eval=TRUE, include=FALSE}
# we create the list of models
model_list <- list( glmnet = mod_glmnet,
                    rpart = mod_rpart,
                    ctree = mod_ctree,
                    rf = mod_ranger)

confusionTable <- function(mod) {
          UseMethod("confusionTable", mod)
}


confusionTable.default <- function (mod) {
    if (!is(mod, "train")) stop("The object selected is not a model")
  
    tidy(confusionMatrix(mod)$table) %>% 
    mutate(modname = paste0(mod[["method"]],"-",mod[["metric"]]) ) %>% 
    spread(modname, n) %>% 
    mutate(Case = c("True Negative", 
                   "False Negative",
                   "False Postive", 
                   "True Positive")) %>% 
    arrange(Case) %>% 
    select(Case, everything()) %>% 
    mutate_if(is.numeric, round, 2)
}

confusionTable.list <- function(mod) {
    map_dfc(mod, confusionTable) %>% 
    select(1:4, seq(8, length(model_list)*4, 4))
}

mod_table <- confusionTable(model_list)

mod_names <- names(mod_table)[4 : ncol(mod_table)] # this is for saving the order of models
  
mod_table

```

Of the 4  models, ```glmnet``` is the one that has the highest rate of detection of *Yes* (True Positive), but also the highest rate of False Allarms (False Positive). The 2 tree based models detect only a very small fraction of True Positive, whereas the Random Forest does not get any hit and always bets for *No*.

## Sensitivity (Recall), Specificity, Recall

The number of True and False predictions can be used to calculate other metrics of the performance of the models. The 

**Sensitivity** AKA **Recall**: True Positive Rate, TruePositives/ (TruePositive + FalseNegatives). It  indicates the percentage of Positives that are detected by the model

**Specificity**: True Negative Rate,  TrueNegatives/ (TrueNegatives + FalsePositives). It indicates the percentage of Negatives identified by the model. 

While the sensitivity informs us on how likely is that a positive case will be detected, it does not tell us how much confident we can be that a positive prediction will be indeed true. This is expressed by the Precision

**Precision**: TruePositives / (TruePositives+FalsePositives). While the Sensitivity informs us on how likely is that a positive case will be detected, it does not tell us how much confident we can be that a positive prediction will be indeed true.

**F**: the harmonic mean of Precision and Recall.

We create the function ```summary_td``` that calculates those metrics and then  ```get_metrics``` that we will use for appling them to the different folds of cross-validated dataset.

```{r calc_metrics}
summary_td <- function(x) {
      pr <- prSummary(x, lev = c("Yes", "No")) %>% 
            t() %>% 
            as.data.frame()
      tc <- twoClassSummary(x, lev = c("Yes", "No")) %>% 
            t() %>% 
            as.data.frame() 
      bind_cols(pr, tc)
}

get_metrics <- function(mod) {

  modname <-  mod[["method"]]
  
  mod[["pred"]] %>% 
  select(obs, pred, Yes, No, Resample) %>%
  # we split the dataframe in a list of dataframes
  split(., .$Resample) %>% 
  # here we apply summary_td to each of the dataframes
  map_dfr(., summary_td, .id = "id") %>% 
  mutate(model = !!modname ) %>%  
  rename(Sensitivity = Sens, Specificity = Spec) %>% 
  # reshape and summarize
  gather("metric","value", 2:(ncol(.)-1)) %>% 
  group_by(model, metric) %>% 
  summarize(mean = mean(value,na.rm =TRUE), sd = sd(value, na.rm =TRUE)) %>% 
  ungroup()
}

metrics <- map_dfr(model_list, get_metrics)
glimpse(metrics)
```

We plot the results.

```{r plot_metrics1, warning=FALSE}
metrics %>% 
    filter(metric %in% c("Sensitivity", "Specificity", "Precision", "F")) %>%
    mutate(metric = factor(metric, levels = c("Sensitivity", "Specificity", "Precision", "F"))) %>% 
    mutate(model = factor(model, levels = mod_names)) %>% 
    ggplot(aes(y= mean, x = model, col = model)) +
    geom_point(size = 2.5) +
    geom_errorbar(aes(ymin = mean - sd, ymax =  mean + sd), width = 0.1) +
    scale_y_continuous(limits = c(0, 1)) + 
    coord_flip() +
    facet_grid(. ~ metric) +
    theme(legend.position = "none",
          panel.spacing = unit(1.2, "lines"))
```
Again, the glmnet model is overall the one that performs better and has highest F.  The tree-based models have very good specificity but poor sensitivity and precision. Overall, all the 4 models have poor performances (i.e. 1).

# Thresholds, ROC curves and Precision-Recall Curves

## Thresholds

Let's take a look at the predictions of one of the model.

```{r }
glimpse(mod_ranger$pred)
```
You  will notice that the model provides for each case a certain probability of Yes and of No. This proabilities are then used to generate the corresponding prediction, with values greater 0.5 cut-off flagged as Yes and  smaller as No. However, there are scenarios in which we might be willing to trade same precision and thus tollerate higher number of False Positives in exchange of an higher sensitivity, becouse for example a False Negative might mean that result in big explosion (e.s. algorithm that identifies mines from sonar data).

The graph and the table above reported display metrics calculated using a cut-off probability of 0.5. A model like the RandomForest that did not predict any "Yes" might be able to get some positive hits, under specific thresholds conditions. This introduce another important parameter and makes everything a little bitmore complicated, right?
What can we do?

To have a better idea of how the models will perform under those different conditions, we can simply vary systematicaly the cut-of probability for assigning the prediction values ("Yes" or "No") and then calculate the metrics above indicated for each of the threshold. 

That is exactly the concept behind the ROC and Precision-Recall Curves.

## ROC curves

[ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curves are generated  by varying the threshold proability and plotting the Sensitivity of the model ( TruePositives/ (TruePositive + FalseNegatives) ) as a function of  1 - Specificity ( TrueNegatives/ (TrueNegatives + FalsePositives) ). In other words, the ROC curve plots the rate of True Positives (over the total positive) as a function of the rate of True Negatives.

To calculate the data of the curves we will use the package ```PRROC```. The [package](https://cran.r-project.org/web/packages/PRROC/vignettes/PRROC.pdf) has same very handy functions to calculate (and plot) ROC and Precision-Recall curves. We create our own function that extracts the data from multiple ````train``` objects using the comands ```PRROC::roc.curve``` and ```PRROC::pr_curve```,  and  then applys it to the same unpacked data-arguments (_*quasiquoted*_) with ``purrr:invoke_map```.

```{r rocpr}
get_rocpr <- function(mod, cv = "pr") {
  # cv = c("pr", "ROC")
  # 
  
  lab <- mod[["pred"]][["obs"]] %>% 
            recode(Yes = 1, No = 0)
  
  # labels go in weightclass https://academic.oup.com/bioinformatics/article/31/15/2595/187781 
  
  # QuasiQuote https://adv-r.hadley.nz/quasiquotation.html
  fr <- list(scores.class0 = mod[["pred"]][["Yes"]], weights.class0 = lab, curve = TRUE)
  
  #invoke_map to apply 2 functions and same quoted arguments to the same data 
  rocpr_data <- exec("invoke_map", list(pr.curve, roc.curve), !!!fr)

  # prec curve using the predition and observations of the model
  pr_data <-  tibble(Recall = rocpr_data[[1]]$curve[, 1],
                     Precision = rocpr_data[[1]]$curve[, 2],
                     Threshold = rocpr_data[[1]]$curve[, 3],
                     modname = mod[["method"]])

  
  roc_data <-   tibble(FPR = rocpr_data[[2]]$curve[, 1],
                       Sensitivity =rocpr_data[[2]]$curve[, 2],
                       Threshold = rocpr_data[[2]]$curve[, 3],
                       modname = mod[["method"]])
  
  if (cv ==  "pr")  return(pr_data) 
  if (cv == "ROC") return(roc_data) 
}

```

We obtain the data of the ROC curves applying our new created ```get_rocpr``` function to the list of models.

```{r get_dataroc}
data_roc <- map_dfr(model_list, get_rocpr, "ROC")

str(data_roc)
```

A perfect model that is able to capture alla the "Yes"  and all the "No" will have Sensitivity = Specificity = 1, and thus  will generate a curve that passes through the upper left corner, continues parallel to the x-axes and reaches the top-right corner. On the other hand, a model that gives random predictions will generate a curve that will lie in the vicinity of the dotted line. Of course we might have also a model that make predictions worse than random, in that case the curve will be to the right of the dotted line. Let's look at the data.

```{r ROC_plot, fig.align="center"}
data_roc  %>% 
  mutate(modname = factor(modname, levels = mod_names)) %>%
  ggplot(aes(y = Sensitivity, x = FPR  , col = modname)) +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = 2) +
  labs( y =  "Sensitivity TP/(TP+FN)",
        x = "1 - Specificity TN/(TN + FP)",
        title = "ROC curves") +
  theme(plot.title = element_text(hjust = 0.5)) 
```
The model glmnet still perform better than all the others, wheras rpart and ctree have predictions very close to pure chance.  The RandomForest performs better than chance but still less than the glmnet. 
With the best model (glmnet), the trade-off for detecting 75% of the true positives (sensitivity=0.75), would be to have 50% chances of making a mistake identifing a negative (1 - Specificity = 0.5)...better than chance but still far from perfect.

## Precision-Recall Curve

A [good argument](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4349800/) has been made that the ROC curve can be misleading when dealing with very unballanced classes. When the positive class is very small, the probability of making False Positive predictions increases. Becouse False Positives might still be a small percentage of  the indentified True Negative, the specificity of the model would be only minimally affected by such class unballance. 

[Precision-Recall](https://ils.unc.edu/courses/2013_spring/inls509_001/lectures/10-EvaluationMetrics.pdf) take that into account and plot the maximal Precision vs Recall (Sensitivity) obtained by varing the probability threshold.

It is important to stress that becouse of this variation in the cut-off threshold, also a model that always assign probability < 0.5 for the class "Yes" (as our RandomForest) might have a good precision when there is a positive probability differential between "Yes" and "No".

Here an example. We create 30 observation:  the first 29 are "No" wheras the last one is "Yes". The probabilities generated by the model are all < 0.5, with the first 29 = 0.4 and the the last one equal 0.41. We can automatically ```plot```the data of class ```PRROC```.

```{r example, fig.align="center"}
# observation are all No except the last one that is Yes
lab <- c(rep(0, 29), 1)

# probability is always < 0.5, 
# but for the last case (the positive obs) 
# is higher than the others (from 0.4 to 0.41)
sc <- c(rep(0.4, 29), 0.41)

cv <-  pr.curve(scores.class0 = sc, weights.class0 = lab, curve = TRUE)
plot(cv)
```
The legend to the right indicate the threshold used to caculate precison and recall. When the threshold is set to 0.41 the predictions of the model are perfect even if originally the probabilities were all <0.5.

OK, we are now ready to plot the Precision-Recall curves of our models.

```{r precision_recal_alt, include=TRUE ,eval=TRUE, fig.align="center"}
data_pr <- map_dfr(model_list, get_rocpr, "pr")


data_pr  %>% 
  mutate(modname = factor(modname, levels = mod_names)) %>%
  ggplot(aes(y = Precision, x = Recall, col = modname)) +
  geom_line() +
  geom_abline(slope = 0, intercept = 4.79/ 100, linetype = 2) +
  labs( y = "Precision TP/(TP+FP)",
        x = "Recall TP/(TP+FN)",
        title = "Precison-Recall Curves") +
  scale_y_continuous(limits = c(0,1)) +
  theme(plot.title = element_text(hjust = 0.5)) 
```  

The rank of the models observed in previous graph is mantained also for the Precision-Recall curves, but the limitations of the models are more evident. The best model, glmnet, reaches a precision of 0.5 (only one over 2 positive prediction is true) 


```{r extr_rocpr2}
get_rocpr2 <- function(mod, out = "curve", resp = "Yes"){
          
          if (!is(mod, "train")) stop("The object selected is not a model")
          
          # we get the prediction from the caret model
          df <- mod[["pred"]]
         
          # we insert scorese (probabilities) and observations and folds
          cvdat <-  mmdata(scores = df$Yes, labels = df$obs, posclass = resp, fold_col = df$Resample)
          
          # it would have been a great code-saver
          # https://goo.gl/Xp1d6t 
          metrics_mod <-  fortify(evalmod(cvdat, mode = "basic")) %>%
                                mutate(modname = mod[["method"]])

          # this is to get the values of ROC and PRC curves
          dat_rocpr <- as.data.frame(evalmod(cvdat)) %>%
                          mutate(modname = mod[["method"]])

          if (out == "curve")  return(dat_rocpr) # this is for the curves
          if (out == "evalmod") return(evalmod(cvdat)) # this is in case we want to get the evaluated model
          if (out == "metrics") return(metrics_mod)  # this is if we want the metrics (but BUG in the package)
}
          
data_rocpr <- map_dfr(model_list, get_rocpr2)

data_rocpr  %>%
  filter(type == "PRC") %>% 
  mutate(modname = factor(modname, levels = mod_names)) %>%
  ggplot(aes(y = y, x = x, col = modname)) +
  geom_line() +
  geom_abline(slope = 0, intercept = 4.79/ 100, linetype = 2) +
  labs( y = "Precision TP/(TP+FP)",
        x = "Recall TP/(TP+FN)",
        title = "Precison-Recall Curves") +
  scale_y_continuous(limits = c(0,1)) +
  theme(plot.title = element_text(hjust = 0.5)) 
```


